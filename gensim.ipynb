{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 10:02:51,442 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "import networkx as nx\n",
    "from collections import defaultdict,namedtuple,Counter\n",
    "from glob import glob\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from six.moves import xrange\n",
    "if sys.version_info[0] >= 3:\n",
    "    unicode = str\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 10:02:52,106 done... 2708 papers loaded\n",
      "2017-02-21 10:02:52,107 7 labels\n"
     ]
    }
   ],
   "source": [
    "CORA = namedtuple('CORA', 'words tags')\n",
    "\n",
    "datasets = []\n",
    "labels = defaultdict(list)\n",
    "with open(\"cora.content\") as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        ID = line[0]\n",
    "        labels[line[-1]].append(ID)\n",
    "        words = []\n",
    "        for i,w in enumerate(line[1:-1]):\n",
    "            if w == \"1\":\n",
    "                words.append(str(i))\n",
    "        datasets.append(\n",
    "            CORA(\n",
    "                words,\n",
    "                [ID]\n",
    "            )\n",
    "        )\n",
    "\n",
    "logging.info(\"done... %s papers loaded\" % (len(datasets)))\n",
    "logging.info(\"%s labels\" % (len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# pretrain doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 10:02:52,128 collecting all words and their counts\n",
      "2017-02-21 10:02:52,129 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-02-21 10:02:52,151 collected 1432 word types and 2708 unique tags from a corpus of 2708 examples and 49216 words\n",
      "2017-02-21 10:02:52,152 Loading a fresh vocabulary\n",
      "2017-02-21 10:02:52,156 min_count=10 retains 968 unique words (67% of original 1432, drops 464)\n",
      "2017-02-21 10:02:52,157 min_count=10 leaves 46251 word corpus (93% of original 49216, drops 2965)\n",
      "2017-02-21 10:02:52,160 deleting the raw counts dictionary of 1432 items\n",
      "2017-02-21 10:02:52,161 sample=0.001 downsamples 76 most-common words\n",
      "2017-02-21 10:02:52,162 downsampling leaves estimated 39952 word corpus (86.4% of prior 46251)\n",
      "2017-02-21 10:02:52,163 estimated required memory for 968 words and 100 dimensions: 2883200 bytes\n",
      "2017-02-21 10:02:52,166 resetting layer weights\n",
      "2017-02-21 10:02:52,214 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:02:52,215 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:02:53,005 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:02:53,012 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:02:53,024 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:02:53,025 training on 246080 raw words (213403 effective words) took 0.8s, 265646 effective words/s\n",
      "2017-02-21 10:02:53,026 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:02:53,029 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:02:53,030 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:02:53,866 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:02:53,889 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:02:53,891 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:02:53,891 training on 246080 raw words (213368 effective words) took 0.9s, 249557 effective words/s\n",
      "2017-02-21 10:02:53,892 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:02:53,895 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:02:53,896 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:02:54,736 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:02:54,741 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:02:54,745 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:02:54,746 training on 246080 raw words (213101 effective words) took 0.8s, 252636 effective words/s\n",
      "2017-02-21 10:02:54,747 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:02:54,750 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:02:54,751 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:02:55,559 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:02:55,564 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:02:55,570 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:02:55,571 training on 246080 raw words (213212 effective words) took 0.8s, 262075 effective words/s\n",
      "2017-02-21 10:02:55,572 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:02:55,576 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:02:55,577 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:02:56,352 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:02:56,361 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:02:56,363 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:02:56,364 training on 246080 raw words (213077 effective words) took 0.8s, 273438 effective words/s\n",
      "2017-02-21 10:02:56,364 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:02:56,368 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:02:56,369 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:02:57,177 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:02:57,203 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:02:57,209 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:02:57,210 training on 246080 raw words (213482 effective words) took 0.8s, 255799 effective words/s\n",
      "2017-02-21 10:02:57,210 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:02:57,214 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:02:57,215 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:02:57,992 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:02:57,994 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:02:58,008 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:02:58,009 training on 246080 raw words (213445 effective words) took 0.8s, 271817 effective words/s\n",
      "2017-02-21 10:02:58,010 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:02:58,013 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:02:58,014 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:02:58,766 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:02:58,793 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:02:58,804 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:02:58,805 training on 246080 raw words (213065 effective words) took 0.8s, 271574 effective words/s\n",
      "2017-02-21 10:02:58,805 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:02:58,809 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:02:58,810 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:02:59,509 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:02:59,543 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:02:59,554 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:02:59,555 training on 246080 raw words (213403 effective words) took 0.7s, 288885 effective words/s\n",
      "2017-02-21 10:02:59,556 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:02:59,559 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:02:59,560 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:03:00,400 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:03:00,412 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:03:00,415 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:03:00,416 training on 246080 raw words (213438 effective words) took 0.8s, 251360 effective words/s\n",
      "2017-02-21 10:03:00,416 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "#model = Doc2Vec(dbow_words=1,iter=5,batch_words=100,negative=20,min_count=0,sample=0.001,dm=0)\n",
    "model = Doc2Vec(alpha=0.025, window=10, min_count=10, min_alpha=0.025, size=100)\n",
    "model.build_vocab(datasets)\n",
    "\n",
    "# decrease alpha\n",
    "for i in range(10):\n",
    "    random.shuffle(datasets)\n",
    "    model.alpha = 0.025-0.002*i\n",
    "    model.min_alpha = model.alpha\n",
    "    model.train(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.611521418021\n",
      "scores 0.668759231905\n"
     ]
    }
   ],
   "source": [
    "# classify with 50% data as training dataset\n",
    "X = []\n",
    "Y = []\n",
    "with open('doc2vec.embd','w') as f:\n",
    "    f.write(\"%s %s\\n\"%(len(datasets),100))\n",
    "    for y,key in enumerate(labels.keys()):\n",
    "        for index,paper in enumerate(labels[key]):\n",
    "            f.write(paper+\" \"+\" \".join([str(x) for x in model.docvecs[paper]])+\"\\n\")\n",
    "            X.append(model.docvecs[paper])\n",
    "            Y.append(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=0)\n",
    "clf = SVC(kernel='rbf',C=1.5).fit(X_train,y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "# classify with 10-fold\n",
    "parameters = {\n",
    "    \"kernel\":[\"rbf\"],\n",
    "    \"C\" :[1.5]\n",
    "             }\n",
    "tunedclf = GridSearchCV(clf,parameters,cv=10,n_jobs=24)\n",
    "tunedclf.fit(X,Y)\n",
    "print(\"scores %s\" % tunedclf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 10:03:05,029 precomputing L2-norms of doc weight vectors\n"
     ]
    }
   ],
   "source": [
    "G = defaultdict(dict)\n",
    "\n",
    "for data in datasets:\n",
    "    for n in model.docvecs.most_similar(data.tags,topn=2):\n",
    "        G[data.tags[0]][n[0]] = None\n",
    "        G[n[0]][data.tags[0]] = None\n",
    "\n",
    "with open('cora.cites') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip().split(\"\\t\")\n",
    "        try:\n",
    "            G[line[0]][line[1]] = None\n",
    "            G[line[1]][line[0]] = None\n",
    "        except:\n",
    "            print(line)\n",
    "\n",
    "neighbors = []\n",
    "\n",
    "# default parameters for deepwalk\n",
    "# 10 iterations\n",
    "for i in range(10):\n",
    "    for node in G:\n",
    "        path = [node]\n",
    "        # 40 walks per node\n",
    "        while len(path) < 40:\n",
    "            cur = path[-1]\n",
    "            path.append(random.choice(list(G[cur].keys())))\n",
    "        neighbors.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 10:03:07,930 collecting all words and their counts\n",
      "2017-02-21 10:03:07,932 PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-02-21 10:03:08,000 PROGRESS: at sentence #10000, processed 400000 words, keeping 2708 word types\n",
      "2017-02-21 10:03:08,069 PROGRESS: at sentence #20000, processed 800000 words, keeping 2708 word types\n",
      "2017-02-21 10:03:08,119 collected 2708 word types from a corpus of 1083200 raw words and 27080 sentences\n",
      "2017-02-21 10:03:08,120 Loading a fresh vocabulary\n",
      "2017-02-21 10:03:08,130 min_count=0 retains 2708 unique words (100% of original 2708, drops 0)\n",
      "2017-02-21 10:03:08,131 min_count=0 leaves 1083200 word corpus (100% of original 1083200, drops 0)\n",
      "2017-02-21 10:03:08,140 deleting the raw counts dictionary of 2708 items\n",
      "2017-02-21 10:03:08,141 sample=0.001 downsamples 4 most-common words\n",
      "2017-02-21 10:03:08,142 downsampling leaves estimated 1075834 word corpus (99.3% of prior 1083200)\n",
      "2017-02-21 10:03:08,143 estimated required memory for 2708 words and 100 dimensions: 3520400 bytes\n",
      "2017-02-21 10:03:08,152 resetting layer weights\n",
      "2017-02-21 10:03:08,185 loading projection weights from doc2vec.embd\n",
      "2017-02-21 10:03:08,424 merged 2708 vectors into (2708, 100) matrix from doc2vec.embd\n",
      "2017-02-21 10:03:08,425 training model with 3 workers on 2708 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-02-21 10:03:08,426 expecting 27080 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:03:09,441 PROGRESS: at 26.59% examples, 1414242 words/s, in_qsize 5, out_qsize 0\n",
      "2017-02-21 10:03:10,442 PROGRESS: at 54.10% examples, 1446183 words/s, in_qsize 5, out_qsize 0\n",
      "2017-02-21 10:03:11,454 PROGRESS: at 81.61% examples, 1451610 words/s, in_qsize 5, out_qsize 0\n",
      "2017-02-21 10:03:12,107 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:03:12,117 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:03:12,120 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:03:12,121 training on 5416000 raw words (5379259 effective words) took 3.7s, 1457489 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5379259"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "p2v = Word2Vec(size=100, window=5, min_count=0)\n",
    "p2v.build_vocab(neighbors)\n",
    "p2v.intersect_word2vec_format('doc2vec.embd')\n",
    "p2v.train(neighbors)\n",
    "#model = Word2Vec.load_word2vec_format('p2v.emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808714918759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 10:03:17,211 scores 0.807607090103\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for y,key in enumerate(labels.keys()):\n",
    "    for index,paper in enumerate(labels[key]):\n",
    "        X.append(p2v[paper])\n",
    "        Y.append(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=42)\n",
    "clf = SVC(kernel='rbf',C=1.5).fit(X_train,y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "clf = SVC()\n",
    "parameters = {\n",
    "    \"kernel\":[\"rbf\"],\n",
    "    \"C\" :[1,10,100]\n",
    "             }\n",
    "tunedclf = GridSearchCV(clf,parameters,cv=10,n_jobs=24)\n",
    "tunedclf.fit(X,Y)\n",
    "logging.info(\"scores %s\" % tunedclf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# predict neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15943\n"
     ]
    }
   ],
   "source": [
    "G = defaultdict(dict)\n",
    "\n",
    "for data in datasets:\n",
    "    for n in model.docvecs.most_similar(data.tags,topn=2):\n",
    "        G[data.tags[0]][n[0]] = None\n",
    "        G[n[0]][data.tags[0]] = None\n",
    "\n",
    "with open('cora.cites') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip().split(\"\\t\")\n",
    "        try:\n",
    "            G[line[0]][line[1]] = None\n",
    "            G[line[1]][line[0]] = None\n",
    "        except:\n",
    "            print(line)\n",
    "\n",
    "        #f.write(\"%s,%s\\n\" % (data.tags[0],n[0]))\n",
    "neighbors = []\n",
    "\n",
    "for node in G:\n",
    "    for neighbor in node:\n",
    "        neighbors.append([node,neighbor])\n",
    "print(len(neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# method in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 10:03:17,686 collecting all words and their counts\n",
      "2017-02-21 10:03:17,688 PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-02-21 10:03:17,696 PROGRESS: at sentence #10000, processed 20000 words, keeping 1713 word types\n",
      "2017-02-21 10:03:17,701 collected 2718 word types from a corpus of 31886 raw words and 15943 sentences\n",
      "2017-02-21 10:03:17,701 Loading a fresh vocabulary\n",
      "2017-02-21 10:03:17,710 min_count=0 retains 2718 unique words (100% of original 2718, drops 0)\n",
      "2017-02-21 10:03:17,711 min_count=0 leaves 31886 word corpus (100% of original 31886, drops 0)\n",
      "2017-02-21 10:03:17,719 deleting the raw counts dictionary of 2718 items\n",
      "2017-02-21 10:03:17,720 sample=0.001 downsamples 10 most-common words\n",
      "2017-02-21 10:03:17,721 downsampling leaves estimated 18484 word corpus (58.0% of prior 31886)\n",
      "2017-02-21 10:03:17,722 estimated required memory for 2718 words and 100 dimensions: 3533400 bytes\n",
      "2017-02-21 10:03:17,731 resetting layer weights\n",
      "2017-02-21 10:03:17,767 loading projection weights from doc2vec.embd\n",
      "2017-02-21 10:03:18,002 merged 2708 vectors into (2718, 100) matrix from doc2vec.embd\n",
      "2017-02-21 10:03:18,004 training model with 3 workers on 2718 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-02-21 10:03:18,005 expecting 15943 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:03:18,167 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:03:18,169 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:03:18,172 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:03:18,173 training on 159430 raw words (92459 effective words) took 0.2s, 575458 effective words/s\n",
      "2017-02-21 10:03:18,173 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92459"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v = Word2Vec(size=100, window=5, min_count=0)\n",
    "p2v.build_vocab(neighbors)\n",
    "p2v.intersect_word2vec_format('doc2vec.embd')\n",
    "p2v.train(neighbors)\n",
    "#model = Word2Vec.load_word2vec_format('p2v.emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.623338257016\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for y,key in enumerate(labels.keys()):\n",
    "    for index,paper in enumerate(labels[key]):\n",
    "        X.append(p2v[paper])\n",
    "        Y.append(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=42)\n",
    "clf = SVC(kernel='rbf',C=1.5).fit(X_train,y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "clf = SVC()\n",
    "parameters = {\n",
    "    \"kernel\":[\"rbf\"],\n",
    "    \"C\" :[1,10,100]\n",
    "             }\n",
    "tunedclf = GridSearchCV(clf,parameters,cv=10,n_jobs=24)\n",
    "tunedclf.fit(X,Y)\n",
    "print(\"scores %s\" % tunedclf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
