{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 10:03:39,272 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "import networkx as nx\n",
    "from collections import defaultdict,namedtuple,Counter\n",
    "from glob import glob\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from six.moves import xrange\n",
    "if sys.version_info[0] >= 3:\n",
    "    unicode = str\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 10:03:39,919 done... 2708 papers loaded\n",
      "2017-02-21 10:03:39,922 7 labels\n"
     ]
    }
   ],
   "source": [
    "CORA = namedtuple('CORA', 'words tags')\n",
    "\n",
    "datasets = []\n",
    "labels = defaultdict(list)\n",
    "with open(\"cora.content\") as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        ID = line[0]\n",
    "        labels[line[-1]].append(ID)\n",
    "        words = []\n",
    "        for i,w in enumerate(line[1:-1]):\n",
    "            if w == \"1\":\n",
    "                words.append(str(i))\n",
    "        datasets.append(\n",
    "            CORA(\n",
    "                words,\n",
    "                [ID]\n",
    "            )\n",
    "        )\n",
    "\n",
    "logging.info(\"done... %s papers loaded\" % (len(datasets)))\n",
    "logging.info(\"%s labels\" % (len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# pretrain doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 10:03:39,940 collecting all words and their counts\n",
      "2017-02-21 10:03:39,941 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-02-21 10:03:39,962 collected 1432 word types and 2708 unique tags from a corpus of 2708 examples and 49216 words\n",
      "2017-02-21 10:03:39,963 Loading a fresh vocabulary\n",
      "2017-02-21 10:03:39,967 min_count=10 retains 968 unique words (67% of original 1432, drops 464)\n",
      "2017-02-21 10:03:39,968 min_count=10 leaves 46251 word corpus (93% of original 49216, drops 2965)\n",
      "2017-02-21 10:03:39,971 deleting the raw counts dictionary of 1432 items\n",
      "2017-02-21 10:03:39,972 sample=0.001 downsamples 76 most-common words\n",
      "2017-02-21 10:03:39,973 downsampling leaves estimated 39952 word corpus (86.4% of prior 46251)\n",
      "2017-02-21 10:03:39,974 estimated required memory for 968 words and 100 dimensions: 2883200 bytes\n",
      "2017-02-21 10:03:39,978 resetting layer weights\n",
      "2017-02-21 10:03:40,035 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:03:40,036 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:03:40,883 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:03:40,892 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:03:40,905 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:03:40,906 training on 246080 raw words (213286 effective words) took 0.9s, 247062 effective words/s\n",
      "2017-02-21 10:03:40,907 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:03:40,910 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:03:40,911 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:03:41,752 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:03:41,766 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:03:41,770 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:03:41,771 training on 246080 raw words (213439 effective words) took 0.9s, 250139 effective words/s\n",
      "2017-02-21 10:03:41,772 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:03:41,775 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:03:41,776 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:03:42,636 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:03:42,638 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:03:42,646 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:03:42,646 training on 246080 raw words (213195 effective words) took 0.9s, 246820 effective words/s\n",
      "2017-02-21 10:03:42,647 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:03:42,651 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:03:42,652 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:03:43,467 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:03:43,477 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:03:43,486 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:03:43,487 training on 246080 raw words (213360 effective words) took 0.8s, 258728 effective words/s\n",
      "2017-02-21 10:03:43,488 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:03:43,491 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:03:43,492 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:03:44,278 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:03:44,307 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:03:44,318 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:03:44,319 training on 246080 raw words (213141 effective words) took 0.8s, 259755 effective words/s\n",
      "2017-02-21 10:03:44,320 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:03:44,323 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:03:44,323 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:03:45,116 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:03:45,138 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:03:45,143 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:03:45,144 training on 246080 raw words (213263 effective words) took 0.8s, 262066 effective words/s\n",
      "2017-02-21 10:03:45,145 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:03:45,148 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:03:45,149 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:03:45,987 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:03:45,991 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:03:45,999 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:03:45,999 training on 246080 raw words (213114 effective words) took 0.8s, 252619 effective words/s\n",
      "2017-02-21 10:03:46,000 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:03:46,003 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:03:46,004 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:03:46,789 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:03:46,816 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:03:46,824 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:03:46,825 training on 246080 raw words (213174 effective words) took 0.8s, 261900 effective words/s\n",
      "2017-02-21 10:03:46,825 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:03:46,829 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:03:46,830 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:03:47,596 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:03:47,608 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:03:47,614 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:03:47,615 training on 246080 raw words (213184 effective words) took 0.8s, 273779 effective words/s\n",
      "2017-02-21 10:03:47,616 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-02-21 10:03:47,619 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-21 10:03:47,620 expecting 2708 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:03:48,416 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:03:48,448 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:03:48,458 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:03:48,459 training on 246080 raw words (213323 effective words) took 0.8s, 256492 effective words/s\n",
      "2017-02-21 10:03:48,459 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "#model = Doc2Vec(dbow_words=1,iter=5,batch_words=100,negative=20,min_count=0,sample=0.001,dm=0)\n",
    "model = Doc2Vec(alpha=0.025, window=10, min_count=10, min_alpha=0.025, size=100)\n",
    "model.build_vocab(datasets)\n",
    "\n",
    "# decrease alpha\n",
    "for i in range(10):\n",
    "    random.shuffle(datasets)\n",
    "    model.alpha = 0.025-0.002*i\n",
    "    model.min_alpha = model.alpha\n",
    "    model.train(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.620384047267\n",
      "scores 0.670605612999\n"
     ]
    }
   ],
   "source": [
    "# classify with 50% data as training dataset\n",
    "X = []\n",
    "Y = []\n",
    "with open('doc2vec.embd','w') as f:\n",
    "    f.write(\"%s %s\\n\"%(len(datasets),100))\n",
    "    for y,key in enumerate(labels.keys()):\n",
    "        for index,paper in enumerate(labels[key]):\n",
    "            f.write(paper+\" \"+\" \".join([str(x) for x in model.docvecs[paper]])+\"\\n\")\n",
    "            X.append(model.docvecs[paper])\n",
    "            Y.append(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=0)\n",
    "clf = SVC(kernel='rbf',C=1.5).fit(X_train,y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "# classify with 10-fold\n",
    "parameters = {\n",
    "    \"kernel\":[\"rbf\"],\n",
    "    \"C\" :[1.5]\n",
    "             }\n",
    "tunedclf = GridSearchCV(clf,parameters,cv=10,n_jobs=24)\n",
    "tunedclf.fit(X,Y)\n",
    "print(\"scores %s\" % tunedclf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 10:03:52,934 precomputing L2-norms of doc weight vectors\n"
     ]
    }
   ],
   "source": [
    "G = defaultdict(dict)\n",
    "\n",
    "for data in datasets:\n",
    "    for n in model.docvecs.most_similar(data.tags,topn=2):\n",
    "        G[data.tags[0]][n[0]] = None\n",
    "        G[n[0]][data.tags[0]] = None\n",
    "\n",
    "with open('cora.cites') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip().split(\"\\t\")\n",
    "        try:\n",
    "            G[line[0]][line[1]] = None\n",
    "            G[line[1]][line[0]] = None\n",
    "        except:\n",
    "            print(line)\n",
    "\n",
    "neighbors = []\n",
    "\n",
    "# default parameters for deepwalk\n",
    "# 10 iterations\n",
    "for i in range(10):\n",
    "    for node in G:\n",
    "        path = [node]\n",
    "        # 40 walks per node\n",
    "        while len(path) < 40:\n",
    "            cur = path[-1]\n",
    "            path.append(random.choice(list(G[cur].keys())))\n",
    "        neighbors.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 10:03:55,778 collecting all words and their counts\n",
      "2017-02-21 10:03:55,780 PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-02-21 10:03:55,844 PROGRESS: at sentence #10000, processed 400000 words, keeping 2708 word types\n",
      "2017-02-21 10:03:55,909 PROGRESS: at sentence #20000, processed 800000 words, keeping 2708 word types\n",
      "2017-02-21 10:03:55,958 collected 2708 word types from a corpus of 1083200 raw words and 27080 sentences\n",
      "2017-02-21 10:03:55,959 Loading a fresh vocabulary\n",
      "2017-02-21 10:03:55,970 min_count=0 retains 2708 unique words (100% of original 2708, drops 0)\n",
      "2017-02-21 10:03:55,971 min_count=0 leaves 1083200 word corpus (100% of original 1083200, drops 0)\n",
      "2017-02-21 10:03:55,980 deleting the raw counts dictionary of 2708 items\n",
      "2017-02-21 10:03:55,981 sample=0.001 downsamples 4 most-common words\n",
      "2017-02-21 10:03:55,982 downsampling leaves estimated 1075390 word corpus (99.3% of prior 1083200)\n",
      "2017-02-21 10:03:55,983 estimated required memory for 2708 words and 100 dimensions: 3520400 bytes\n",
      "2017-02-21 10:03:55,991 resetting layer weights\n",
      "2017-02-21 10:03:56,023 loading projection weights from doc2vec.embd\n",
      "2017-02-21 10:03:56,256 merged 2708 vectors into (2708, 100) matrix from doc2vec.embd\n",
      "2017-02-21 10:03:56,257 training model with 3 workers on 2708 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-02-21 10:03:56,258 expecting 27080 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:03:57,272 PROGRESS: at 26.77% examples, 1426469 words/s, in_qsize 5, out_qsize 0\n",
      "2017-02-21 10:03:58,272 PROGRESS: at 54.28% examples, 1452324 words/s, in_qsize 6, out_qsize 0\n",
      "2017-02-21 10:03:59,277 PROGRESS: at 81.61% examples, 1455758 words/s, in_qsize 5, out_qsize 0\n",
      "2017-02-21 10:03:59,933 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:03:59,942 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:03:59,946 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:03:59,947 training on 5416000 raw words (5377107 effective words) took 3.7s, 1459529 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5377107"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "p2v = Word2Vec(size=100, window=5, min_count=0)\n",
    "p2v.build_vocab(neighbors)\n",
    "p2v.intersect_word2vec_format('doc2vec.embd')\n",
    "p2v.train(neighbors)\n",
    "#model = Word2Vec.load_word2vec_format('p2v.emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822008862629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 10:04:04,687 scores 0.820901033973\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for y,key in enumerate(labels.keys()):\n",
    "    for index,paper in enumerate(labels[key]):\n",
    "        X.append(p2v[paper])\n",
    "        Y.append(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=42)\n",
    "clf = SVC(kernel='rbf',C=1.5).fit(X_train,y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "clf = SVC()\n",
    "parameters = {\n",
    "    \"kernel\":[\"rbf\"],\n",
    "    \"C\" :[1,10,100]\n",
    "             }\n",
    "tunedclf = GridSearchCV(clf,parameters,cv=10,n_jobs=24)\n",
    "tunedclf.fit(X,Y)\n",
    "logging.info(\"scores %s\" % tunedclf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# predict neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15943\n"
     ]
    }
   ],
   "source": [
    "G = defaultdict(dict)\n",
    "\n",
    "for data in datasets:\n",
    "    for n in model.docvecs.most_similar(data.tags,topn=2):\n",
    "        G[data.tags[0]][n[0]] = None\n",
    "        G[n[0]][data.tags[0]] = None\n",
    "\n",
    "with open('cora.cites') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip().split(\"\\t\")\n",
    "        try:\n",
    "            G[line[0]][line[1]] = None\n",
    "            G[line[1]][line[0]] = None\n",
    "        except:\n",
    "            print(line)\n",
    "\n",
    "        #f.write(\"%s,%s\\n\" % (data.tags[0],n[0]))\n",
    "neighbors = []\n",
    "\n",
    "for node in G:\n",
    "    for neighbor in node:\n",
    "        neighbors.append([node,neighbor])\n",
    "print(len(neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# method in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 10:04:05,145 collecting all words and their counts\n",
      "2017-02-21 10:04:05,146 PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-02-21 10:04:05,153 PROGRESS: at sentence #10000, processed 20000 words, keeping 1711 word types\n",
      "2017-02-21 10:04:05,158 collected 2718 word types from a corpus of 31886 raw words and 15943 sentences\n",
      "2017-02-21 10:04:05,159 Loading a fresh vocabulary\n",
      "2017-02-21 10:04:05,167 min_count=0 retains 2718 unique words (100% of original 2718, drops 0)\n",
      "2017-02-21 10:04:05,168 min_count=0 leaves 31886 word corpus (100% of original 31886, drops 0)\n",
      "2017-02-21 10:04:05,176 deleting the raw counts dictionary of 2718 items\n",
      "2017-02-21 10:04:05,177 sample=0.001 downsamples 10 most-common words\n",
      "2017-02-21 10:04:05,178 downsampling leaves estimated 18484 word corpus (58.0% of prior 31886)\n",
      "2017-02-21 10:04:05,179 estimated required memory for 2718 words and 100 dimensions: 3533400 bytes\n",
      "2017-02-21 10:04:05,187 resetting layer weights\n",
      "2017-02-21 10:04:05,223 loading projection weights from doc2vec.embd\n",
      "2017-02-21 10:04:05,458 merged 2708 vectors into (2718, 100) matrix from doc2vec.embd\n",
      "2017-02-21 10:04:05,459 training model with 3 workers on 2718 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-02-21 10:04:05,460 expecting 15943 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-21 10:04:05,626 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-21 10:04:05,628 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-21 10:04:05,631 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-21 10:04:05,632 training on 159430 raw words (92376 effective words) took 0.2s, 560497 effective words/s\n",
      "2017-02-21 10:04:05,633 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92376"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v = Word2Vec(size=100, window=5, min_count=0)\n",
    "p2v.build_vocab(neighbors)\n",
    "p2v.intersect_word2vec_format('doc2vec.embd')\n",
    "p2v.train(neighbors)\n",
    "#model = Word2Vec.load_word2vec_format('p2v.emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.599704579025\n",
      "scores 0.706425406204\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for y,key in enumerate(labels.keys()):\n",
    "    for index,paper in enumerate(labels[key]):\n",
    "        X.append(p2v[paper])\n",
    "        Y.append(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=42)\n",
    "clf = SVC(kernel='rbf',C=1.5).fit(X_train,y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "clf = SVC()\n",
    "parameters = {\n",
    "    \"kernel\":[\"rbf\"],\n",
    "    \"C\" :[1,10,100]\n",
    "             }\n",
    "tunedclf = GridSearchCV(clf,parameters,cv=10,n_jobs=24)\n",
    "tunedclf.fit(X,Y)\n",
    "print(\"scores %s\" % tunedclf.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
